A transformer is a specific kind of neural network and machine learning model, and it is the core invention underlying the current boom in AI.   
One of the classic example is **ChatGPT**, stands for **Generative Pre-trained Transformer**. 

There could be different types of transformers:
- Voice to text
- Text to voice
- Text to image
- Image to text

Here the main focus will be the text-generating transformer. 

The transformer reply heavily on **linear algebra** and **probability prediction model**. 
The process has four main parts: **Embedding**, **Attention** **MLPs**, and **Unembedding**. 


##### Embedding
**Token** is a small chunk of data, it can be a piece of text, voice, etc. With each of the token there is a **matrix** links to it. Imagine this matrix points a vector in high-dimensional space, then word with the similar meaning will be closer to each other. 

##### Attention
This process allow each vector talk to each other and update their values. This process is responsible for figuring out which words in context are relevant to updating the meanings of which other words, and how exactly those meanings should be updated. 

##### Multilayer Perceptron

##### Unembedding 


**Softmax**

Action value
Exploring / exploiting dillema
epsilon-greedy test

non-stationary problem

upper-confidence bound

greedification

Bellman equation

SARSA vs Q-Learning

Dense reward, spars reward
considering 

gamma - confidence in looking into future / significance of the future states

Some questions 
1. Why is the reinforcement learning needed when there's supervised learning?
   Not sure about good policy / no proper training set
   AlphaGo has a new opening no human has ever played
2. Is reward in policy reinforcement same as heuristic in A*?
3. 


Difficulty:
so many layers, unstable -> batch normalisation (activation = 0)

ReLu function
sigmoid

Multi-arm bandit machine



Some cool examples:
Atari
AlphaGO
##### Common algorithms
SARSA
Q-Learning